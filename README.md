# Weather Data Engineering API

A comprehensive data engineering solution for ingesting, analyzing, and serving weather data through a REST API.

## üå§Ô∏è Overview

This project implements a complete data pipeline for weather station data, featuring:

- **Data Ingestion**: Efficient processing of weather station files with duplicate detection
- **Data Analysis**: Automated calculation of yearly weather statistics
- **REST API**: High-performance API with filtering, pagination, and auto-documentation
- **Cloud Ready**: Designed for AWS deployment with Infrastructure as Code

## üìä Data Sources

- **Weather Data**: Historical records from 1985-2014 across Nebraska, Iowa, Illinois, Indiana, and Ohio
- **Crop Yield Data**: Corn yield statistics for agricultural correlation analysis

## üèóÔ∏è Architecture

### Data Models
- `WeatherStation`: Station metadata and geographic information
- `DailyWeather`: Raw weather observations with temperature and precipitation
- `YearlyWeatherStats`: Aggregated statistics for efficient querying
- `CropYield`: Agricultural yield data for correlation analysis

### Technology Stack
- **Database**: PostgreSQL with optimized indexing strategy
- **API Framework**: FastAPI with automatic OpenAPI documentation
- **ORM**: Django ORM for robust data modeling
- **Containerization**: Docker with multi-stage builds
- **Infrastructure**: Terraform for AWS deployment

## üöÄ Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL 15+ (optional: Docker for full stack)

### Local Development (Recommended)

1. **Clone and setup**
   ```bash
   git clone https://github.com/smusali/code-challenge-template.git
   cd code-challenge-template
   ```

2. **Create virtual environment**
   ```bash
   # Using venv
   python3.11 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   
   # Or using virtualenv
   pip install virtualenv
   virtualenv venv
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

4. **Test Django setup**
   ```bash
   PYTHONPATH=. python core_django/manage.py check
   ```

5. **Setup environment**
   ```bash
   cp .env.example .env
   # Edit .env with your database credentials
   ```

### Using Poetry (Alternative)

```bash
pip install poetry
poetry install
poetry shell
```

### Using Docker (For full stack)

```bash
docker-compose up --build
```

## üîß Development Commands

### Local Development
```bash
# Activate environment
source venv/bin/activate

# Run Django commands
PYTHONPATH=. python core_django/manage.py check
PYTHONPATH=. python core_django/manage.py shell

# Run tests
pytest

# Code formatting
black .
isort .
ruff check .
```

### With Poetry
```bash
poetry run python core_django/manage.py check
poetry run pytest
poetry run black .
```

### With Docker
```bash
docker-compose exec web python core_django/manage.py check
docker-compose run --rm web pytest
```

## üß™ Testing

### Run Tests Locally
```bash
# With venv
source venv/bin/activate
pytest tests/

# With Poetry
poetry run pytest

# With Docker
docker-compose run --rm web pytest
```

## üìà Performance Characteristics

- **Ingestion Rate**: ~10,000 records/second
- **API Response Time**: <100ms for paginated queries
- **Database Size**: ~50MB for complete dataset
- **Memory Usage**: <100MB during ingestion

## üèóÔ∏è Project Structure

```
code-challenge-template/
‚îú‚îÄ‚îÄ core_django/          # Django ORM models
‚îÇ   ‚îú‚îÄ‚îÄ core/             # Django settings and config
‚îÇ   ‚îú‚îÄ‚îÄ models/           # Data models
‚îÇ   ‚îú‚îÄ‚îÄ manage.py         # Django management
‚îÇ   ‚îî‚îÄ‚îÄ setup.py          # FastAPI integration
‚îú‚îÄ‚îÄ weather_api/          # FastAPI application (future)
‚îú‚îÄ‚îÄ scripts/              # Data processing scripts (future)
‚îú‚îÄ‚îÄ tests/                # Test suites
‚îú‚îÄ‚îÄ docker/               # Container definitions
‚îú‚îÄ‚îÄ wx_data/              # Weather station files
‚îî‚îÄ‚îÄ yld_data/             # Crop yield data
```

## üîß Development Tools

### Code Quality
```bash
# Format code
black .
isort .

# Lint code
ruff check .
mypy .

# Pre-commit hooks
pre-commit install
pre-commit run --all-files
```

## üê≥ Docker Services

### Development Stack
```bash
# Essential services only
docker-compose up

# With monitoring
docker-compose --profile monitoring up

# Full stack
docker-compose --profile monitoring --profile production up
```

## üîç Troubleshooting

### Virtual Environment Issues
```bash
# Create fresh environment
rm -rf venv
python3.11 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### Django Issues
```bash
# Check Django setup
PYTHONPATH=. python core_django/manage.py check

# Django shell
PYTHONPATH=. python core_django/manage.py shell
```

### Poetry Issues
```bash
# Clear cache
poetry cache clear --all pypi
poetry install
```

## üöÄ Cloud Deployment

The application is designed for AWS deployment using:
- **AWS Lambda** + API Gateway for serverless API
- **Amazon Aurora PostgreSQL** for managed database
- **AWS Batch** for scheduled data ingestion
- **EventBridge** for cron scheduling
- **CloudWatch** for monitoring and logs

See `infrastructure/` directory for Terraform configurations.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'feat: add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

### Code Quality Standards
- Follow PEP 8 style guidelines
- Add type hints to all functions
- Include docstrings for public APIs
- Maintain test coverage >90%
- Use conventional commit messages

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Weather data provided by [Corteva](https://github.com/corteva/code-challenge-template)
- Built for the Corteva Data Engineering Challenge
- Inspired by modern data engineering best practices

---

**Note**: This is a demonstration project showcasing data engineering capabilities including ETL pipelines, API development, and cloud architecture design.
